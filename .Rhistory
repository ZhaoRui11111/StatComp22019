}
}
x <- x[,1:n]
rej <- mean(p)/n
return(list(x,rej))
}
sigma <- c(0.1,0.5,2,16)
result1<- laplace.chain(sigma[1])
result2<- laplace.chain(sigma[2])
result3<- laplace.chain(sigma[3])
result4<- laplace.chain(sigma[4])
x1 <- result1[[1]]
reject <- numeric(0)
reject[1] <- result1[[2]]
x2 <- result2[[1]]
reject[2] <- result2[[2]]
x3 <- result3[[1]]
reject[3] <- result3[[2]]
x4 <- result4[[1]]
reject[4] <- result4[[2]]
print(reject)
#initialize constants and parameters
set.seed(111)
N <- 500
n <- 0
burn <- 200 #burn-in length
k <- 5
x <- matrix(0, k, N)
y <- matrix(0, k, N)
rho <- 0.9 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2
###### generate the chain #####
x[,1] <- c(0,-1,0,1,0)
y[,1] <- c(0,0,-1,0,1)#initialize
for (i in 2:N) {
for (j in 1:k) {
yt <- y[j,i-1]
m1 <- mu1 + rho * (yt - mu2) * sigma1/sigma2
x[j,i] <- rnorm(1, m1, s1)
xt <- x[j,i]
m2 <- mu2 + rho * (xt - mu1) * sigma2/sigma1
y[j,i] <- rnorm(1, m2, s2)
}
psi <- t(apply(x[,1:i]+y[,1:i], 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
#print(Gelman.Rubin(psi))
n <- n+1
if(Gelman.Rubin(psi)<1.2){
break
}
}
b <- burn + 1
x <- x[1,b:n]
y <- y[1,b:n]
plot(x,y)
J <- lm(y ~ x)
ks.test(J$residuals,"pnorm")
library(car)
ncvTest(lm(y ~ x))
#data generation
model <- function(alpha,beta,gamma,N){
e_M <- rnorm(N,0,1)
e_Y <- rnorm(N,0,1)
x <- rnorm(N,0,1)
m <- alpha*x+e_M
y <- beta*m+gamma*x+e_Y
return(data.frame(x,m,y))
}
t <- function(x,m,y){              #Test statistics
data <- data.frame(x,m,y)
f1 <- lm(m~x,data=data)
f2 <- lm(y~m+x,data=data)
s_alpha <- summary(f1)$coefficients[2,2]
s_beta <- summary(f2)$coefficients[2,2]
t <- f1$coef[2]*f2$coef[2]/sqrt(f1$coef[2]^2*s_alpha^2+f2$coef[2]^2*s_beta^2)
return(as.numeric(t))
}
N <- 50
R <- 499
K <- 1:N
test1 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
x <- data[k,1]
reps[i] <- t(x,data$m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test2 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
y <- data[k,3]
reps[i] <- t(data$x,data$m,y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test3 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
m <- data[k,2]
reps[i] <- t(data$x,m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
stimulation <- function(alpha,beta,gamma=1,N=50){
type_1_error <- matrix(0,nrow = 3,ncol = 200)
for (i in 1:200) {
data <- model(alpha,beta,gamma,N)
type_1_error[1,i] <- test1(data)
type_1_error[2,i] <- test2(data)
type_1_error[3,i] <- test3(data)
}
return(c(mean(type_1_error[1,]),mean(type_1_error[2,]),mean(type_1_error[3,])))
}
s1 <- stimulation(alpha=0,beta=0)
s2 <- stimulation(alpha=0,beta=1)
N <- 20
R <- 299
K <- 1:N
test1 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
x <- data[k,1]
reps[i] <- t(x,data$m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test2 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
y <- data[k,3]
reps[i] <- t(data$x,data$m,y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test3 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
m <- data[k,2]
reps[i] <- t(data$x,m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
#data generation
model <- function(alpha,beta,gamma,N){
e_M <- rnorm(N,0,1)
e_Y <- rnorm(N,0,1)
x <- rnorm(N,0,1)
m <- alpha*x+e_M
y <- beta*m+gamma*x+e_Y
return(data.frame(x,m,y))
}
t <- function(x,m,y){              #Test statistics
data <- data.frame(x,m,y)
f1 <- lm(m~x,data=data)
f2 <- lm(y~m+x,data=data)
s_alpha <- summary(f1)$coefficients[2,2]
s_beta <- summary(f2)$coefficients[2,2]
t <- f1$coef[2]*f2$coef[2]/sqrt(f1$coef[2]^2*s_alpha^2+f2$coef[2]^2*s_beta^2)
return(as.numeric(t))
}
N <- 20
R <- 299
K <- 1:N
test1 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
x <- data[k,1]
reps[i] <- t(x,data$m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test2 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
y <- data[k,3]
reps[i] <- t(data$x,data$m,y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test3 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
m <- data[k,2]
reps[i] <- t(data$x,m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
stimulation <- function(alpha,beta,gamma=1,N=20){
type_1_error <- matrix(0,nrow = 3,ncol = 100)
for (i in 1:100) {
data <- model(alpha,beta,gamma,N)
type_1_error[1,i] <- test1(data)
type_1_error[2,i] <- test2(data)
type_1_error[3,i] <- test3(data)
}
return(c(mean(type_1_error[1,]),mean(type_1_error[2,]),mean(type_1_error[3,])))
}
s1 <- stimulation(alpha=0,beta=0)
s2 <- stimulation(alpha=0,beta=1)
s3 <- stimulation(alpha=1,beta=0)
#data generation
model <- function(alpha,beta,gamma,N){
e_M <- rnorm(N,0,1)
e_Y <- rnorm(N,0,1)
x <- rnorm(N,0,1)
m <- alpha*x+e_M
y <- beta*m+gamma*x+e_Y
return(data.frame(x,m,y))
}
t <- function(x,m,y){              #Test statistics
data <- data.frame(x,m,y)
f1 <- lm(m~x,data=data)
f2 <- lm(y~m+x,data=data)
s_alpha <- summary(f1)$coefficients[2,2]
s_beta <- summary(f2)$coefficients[2,2]
t <- f1$coef[2]*f2$coef[2]/sqrt(f1$coef[2]^2*s_alpha^2+f2$coef[2]^2*s_beta^2)
return(as.numeric(t))
}
N <- 10
R <- 199
K <- 1:N
test1 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
x <- data[k,1]
reps[i] <- t(x,data$m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test2 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
y <- data[k,3]
reps[i] <- t(data$x,data$m,y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
test3 <- function(data){
reps <- numeric(R)
for (i in 1:R) {
k <- sample(K,size = N,replace = FALSE)
m <- data[k,2]
reps[i] <- t(data$x,m,data$y)
}
p_hat <- (sum(abs(reps)>abs(t(data$x,data$m,data$y)))+1)/(1+R)
return(p_hat<0.05)
}
stimulation <- function(alpha,beta,gamma=1,N=10){
type_1_error <- matrix(0,nrow = 3,ncol = 50)
for (i in 1:50) {
data <- model(alpha,beta,gamma,N)
type_1_error[1,i] <- test1(data)
type_1_error[2,i] <- test2(data)
type_1_error[3,i] <- test3(data)
}
return(c(mean(type_1_error[1,]),mean(type_1_error[2,]),mean(type_1_error[3,])))
}
s1 <- stimulation(alpha=0,beta=0)
s2 <- stimulation(alpha=0,beta=1)
s3 <- stimulation(alpha=1,beta=0)
f <- function(f0,N=10^6,b1=0,b2=1,b3=-1){
x1 <- rpois(N,1)
x2 <- rexp(N,1)
x3 <- rbeta(N,1,0.5)
g <- function(alpha){
tmp <- exp(alpha+b1*x1+b2*x2+b3*x3)
p <- 1/(1+tmp)
mean(p) - f0
}
solution <- uniroot(g,c(-100,100))
alpha <- solution$root
return(alpha)
}
f0 <- c(0.1,0.01,0.001,0.0001)
alpha <- numeric(4)
for (i in 1:4) {
alpha[i] <- f(f0[i])
}
plot(-log(f0),alpha)
#data
U <- c(11,8,27,13,16,0,23,10,24,2)
V <- c(12,9,28,14,17,1,24,11,25,3)
# maximizing likelihood function
likelihood <- function(lambda){
return(sum((V*exp(-lambda*V)-U*exp(-lambda*U))/(exp(-lambda*U)-exp(-lambda*V))))
}
round(uniroot(likelihood, c(0, 10))$root, 5)
#EM-algorithm
lambda <- 3
lambda_t <- 2
i <- 0
eps <- 1e-3
while(1){
lambda_t <- lambda_t - likelihood(lambda)/length(U)
if (abs(1/lambda_t - lambda) < eps) break
lambda <- 1/lambda_t
i <- i+1
}
lambda
gibbsR <- function(N){
X <- matrix(0, N, 2) #the chain, a bivariate sample
rho <- 0.9 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2
X[1, ] <- c(mu1, mu2) #initialize
for (i in 2:N) {
x2 <- X[i-1, 2]
m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
X[i, 1] <- rnorm(1, m1, s1)
x1 <- X[i, 1]
m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
X[i, 2] <- rnorm(1, m2, s2)
}
return(X)
}
library(Rcpp)
cppFunction('NumericMatrix gibbsC(int N) {
double rho = 0.9;
double mu1 = 0;
double mu2 = 0;
double sigma1 = 1;
double sigma2 = 1;
double s1 = 0.4358899;
double s2 = 0.4358899;
NumericMatrix X(N, 2);
X(0,0) = mu1;
X(0,1) = mu2;
for (int i=1;i<N;i++) {
double x2 = X(i-1,1);
double m1 = mu1 + rho * (x2 - mu2) * sigma1/sigma2;
X(i,0) = rnorm(1, m1, s1)[0];
double x1 = X(i,0);
double m2 = mu2 + rho * (x1 - mu1) * sigma2/sigma1;
X(i,1) = rnorm(1, m2, s2)[0];
}
return(X);
}')
GibbsR <- gibbsR(2000)
GibbsC <- gibbsC(2000)
qqplot(GibbsR,GibbsC)
library(StatComp22019)
library(microbenchmark)
data(document)
document <- document[1:50]
usethis::use_data(document)
usethis::use_data(document)
library(StatComp22019)
library(microbenchmark)
data(document)
lda_pack <- function(data,K){
vid<-Corpus(VectorSource(data))
vid<-tm_map(vid,stripWhitespace)
#创建文本-词条矩阵
dtm <- DocumentTermMatrix(vid,
control = list(
wordLengths=c(2, Inf),               #限制词长
bounds = list(global = c(5,Inf)),    #设置词的最小频率
removeNumbers = TRUE,                #是否移除数字
weighting = weightTf,                #词频率权重
encoding = "UTF-8"))
Gibbs = LDA(dtm,k = K,method = "Gibbs")
return(Gibbs)
}
tm2 <- microbenchmark(
ldar = LDAR(document,13,10),
ldap = lda_pack(document,13)
)
tm2 <- microbenchmark(
ldar = LDAR(document,7,10),
ldap = lda_pack(document,7)
)
document <- document[1:10]
usethis::use_data(document)
ldar = LDAR(document,9,10)
devtools::document()
devtools::check()
devtools::check()
library(StatComp22019)
library(microbenchmark)
data(document)
ldar = LDAR(document,5,10)
View(ldar)
library(jiebaR)#分词处理工具
library(lda)#LDA模型包
library(LDAvis)#LDA可视化
library(servr)
#library(Rcpp)
data <- readLines("sohu.txt",encoding = "UTF-8")
index <- sample(1:length(data),100)
data <- data[index]
stop_words<-readLines("stopwords.txt",encoding = "UTF-8")
wk = worker(stop_word='stopwords.txt',bylines = T)
fenci <- wk[data]
data2=gsub('[",c()-]', '', fenci)   #分词以及删除停词
library(jiebaR)#分词处理工具
library(lda)#LDA模型包
library(LDAvis)#LDA可视化
library(servr)
#library(Rcpp)
data <- readLines("sohu.txt",encoding = "UTF-8")
index <- sample(1:length(data),30)
data <- data[index]
stop_words<-readLines("stopwords.txt",encoding = "UTF-8")
wk = worker(stop_word='stopwords.txt',bylines = T)
fenci <- wk[data]
data2=gsub('[",c()-]', '', fenci)   #分词以及删除停词
document <- data2
usethis::use_data(document)
devtools::build_vignettes()
?Corpus
?tm_map
?DocumentTermMatrix
devtools::build_vignettes()
devtools::build_vignettes()
?LDA
detach("package:topicmodels", unload = TRUE)
library(topicmodels)
devtools::build_vignettes()
?LDAR
devtools::build_vignettes()
devtools::build_vignettes()
library(tm)
library(StatComp22019)
library(microbenchmark)
library(topicmodels)
data(document)
lda_pack <- function(data,K){
vid<-Corpus(VectorSource(data))
vid<-tm_map(vid,stripWhitespace)
#创建文本-词条矩阵
dtm <- DocumentTermMatrix(vid,
control = list(
wordLengths=c(2, Inf),               #限制词长
bounds = list(global = c(5,Inf)),    #设置词的最小频率
removeNumbers = TRUE,                #是否移除数字
weighting = weightTf,                #词频率权重
encoding = "UTF-8"))
Gibbs = LDA(dtm,k = K,method = "Gibbs")
return(Gibbs)
}
tm2 <- microbenchmark(
ldar = LDAR(document,9,10),
ldap = lda_pack(document,9)
)
devtools::build_vignettes()
?LDAR
LDAR(document,9,10)
devtools::build_vignettes()
devtools::document()
devtools::check()
devtools::check()
devtools::build_vignettes()
LDAR<- function(data,K,n){
vid<-Corpus(VectorSource(data))
vid<-tm_map(vid,stripWhitespace)
#创建文本-词条矩阵
dtm <- DocumentTermMatrix(vid,
control = list(
wordLengths=c(2, Inf),
bounds = list(global = c(5,Inf)),
removeNumbers = TRUE,
weighting = weightTf,
encoding = "UTF-8"))
Gibbs = LDA(dtm,k = K,method = "Gibbs")
return(terms(Gibbs,n))
}
LDAR(document,9)
LDAR(document,9,10)
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build(vignettes=FALSE)
install.packages('../StatComp22019_1.0.tar.gz',repo=NULL)
devtools::document()
devtools::check()
devtools::check()
devtools::build_vignettes()
devtools::build(vignettes=FALSE)
